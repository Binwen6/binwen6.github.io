<!DOCTYPE html>
<html>
<head>
<title>顶会顶刊之EEG下游任务.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E9%92%88%E5%AF%B9neurips-iclr-tpami-icassp%E7%AD%89%E9%A1%B6%E4%BC%9A%E9%A1%B6%E5%88%8A%E7%9A%84%E8%B0%83%E7%A0%94%E6%9C%89%E5%85%B3%E9%9F%B3%E9%A2%91%E5%9B%BE%E5%83%8F%E4%BF%A1%E5%8F%B7%E5%88%86%E7%B1%BB%E4%B8%8E%E9%87%8D%E5%BB%BA%E7%9A%84eeg%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E4%BB%A5%E5%8F%8A%E5%8F%AF%E7%94%A8%E4%BA%8Elinear-probing%E5%BE%AE%E8%B0%83%E7%9A%84eeg-foundation-model">针对NeurIPS, ICLR, TPAMI, ICASSP等顶会顶刊的调研———有关音频、图像信号分类与重建的EEG下游任务以及可用于Linear Probing微调的EEG Foundation Model</h1>
<h3 id="%F0%9F%93%98-%E9%98%85%E8%AF%BB%E8%AF%B4%E6%98%8E">📘 阅读说明：</h3>
<blockquote>
<p>本表系统整理了自 2022 年以来发表于 NeurIPS、ICLR、TPAMI、ICASSP 等顶级会议/期刊中，与脑电（EEG）下游任务相关的重要研究工作，聚焦于音频、图像、语音等模态的分类与重建任务，以及可用于线性探测（Linear Probing）微调的 EEG 基础模型（Foundation Model）。表格包含方法名、出处、方法简介、主要贡献、适用任务、引用量、开源代码等关键信息，便于快速查阅、对比与复现。</p>
</blockquote>
<blockquote>
<p>其中，蓝色标注的方法复现意义较强，建议优先关注；橙色标注为高质量竞赛任务。</p>
</blockquote>
<table>
<thead>
<tr>
<th>序号</th>
<th>方法名</th>
<th>出处</th>
<th>文献标题</th>
<th>方法简介</th>
<th>主要贡献</th>
<th>适用任务</th>
<th>引用量</th>
<th>开源代码</th>
<th>论文链接</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>MAtt</td>
<td>NeurIPS 2022</td>
<td>MAtt: A Manifold Attention Network for EEG Decoding</td>
<td>基于SPD流形的注意力网络，学习EEG的时空表示</td>
<td>在同步/异步EEG任务上表现优秀，结构具可解释性</td>
<td>EEG解码</td>
<td>43</td>
<td>✅ <a href="https://github.com/cecnl/matt">GitHub</a>(65)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c981fd12b1d5703f19bd8289da9fc996-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>2</td>
<td>SPDDSMBN/TSMNet</td>
<td>NeurIPS 2022</td>
<td>SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG</td>
<td>提出SPD(几何深度学习)域专属BN层，实现EEG的领域不变特征提取</td>
<td>支持多源/目标UDA(无监督领域适应)，首次实现EEG端到端无手工映射迁移</td>
<td>EEG跨域迁移</td>
<td>58</td>
<td>✅ <a href="https://github.com/rkobler/TSMNet">GitHub</a>(58)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/28ef7ee7cd3e03093acc39e1272411b7-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>3</td>
<td>EEG-LatentSpace</td>
<td>NeurIPS 2022</td>
<td>Evaluating Latent Space Robustness and Uncertainty of EEG-ML under Shifts</td>
<td>对EEG潜在空间稳定性与模型不确定性进行诊断</td>
<td>在临床EEG任务中揭示分布偏移对模型泛化的影响</td>
<td>临床EEG解码</td>
<td>11</td>
<td>✅<a href="https://github.com/neerajwagh/evaluating-eeg-representations">GitHub</a>(16)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8511d06d5590f4bda24d42087802cc81-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>4</td>
<td>MAEEG (SSL)</td>
<td>NeurIPS 2022</td>
<td>MAEEG: Masked Auto-encoder for EEG Representation Learning</td>
<td>Transformer + 掩码预测，进行自监督学习</td>
<td>在睡眠分期任务中表现显著优于传统方法</td>
<td>EEG睡眠阶段分类</td>
<td>49</td>
<td>❌</td>
<td><a href="https://neurips.cc/virtual/2022/60053">链接</a></td>
</tr>
<tr>
<td>5</td>
<td>MMM</td>
<td>NeurIPS 2023</td>
<td>Learning Topology‑Agnostic EEG Representations with Geometry‑Aware Modeling</td>
<td>多级通道融合 + 电极统一嵌入空间</td>
<td>可适配不同数据集电极配置，实现跨数据集训练</td>
<td>EEG情绪识别</td>
<td>未知</td>
<td>✅ <a href="https://seqml.github.io/MMM/">项目页</a></td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a8c893712cb7858e49631fb03c941f8d-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>6</td>
<td><span style="color:blue;"> DeWave</span></td>
<td>NeurIPS 2023</td>
<td>DeWave: Discrete Encoding of EEG Waves for EEG to Text Translation</td>
<td>使用VQ-VAE编码EEG，映射到语言模型空间</td>
<td>首次实现无眼动标记的EEG转文本任务</td>
<td>EEG→语言生成</td>
<td>34</td>
<td>✅ <a href="https://github.com/duanyiqun/DeWave">GitHub</a>(174)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1f2fd23309a5b2d2537d063b29ec1b52-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>7</td>
<td><span style="color:blue;">EEGPT</span></td>
<td>NeurIPS 2024</td>
<td>EEGPT: Pretrained Transformer for Universal and Reliable Representation of EEG Signals</td>
<td>双掩码Transformer模型，10M参数，在大规模任务上预训练</td>
<td>支持多个EEG下游任务，训练高效，适配性强，在多种下游任务中通过线性探测法实现了最先进性能。</td>
<td>通用EEG表示</td>
<td>未知</td>
<td>✅ <a href="https://github.com/BINE022/EEGPT">GitHub</a>(160)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/4540d267eeec4e5dbd9dae9448f0b739-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>8</td>
<td><span style="color:blue;">  EEG视觉扩散</span></td>
<td>NeurIPS 2024</td>
<td>Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion</td>
<td>ATM编码EEG至CLIP语义空间，引导扩散图像重建</td>
<td>视觉零样本生成首次在EEG领域中实现，性能优</td>
<td>EEG→图像生成</td>
<td>24</td>
<td>✅ <a href="https://github.com/dongyangli-del/EEG_Image_decode">GitHub</a>(144)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/ba5f1233efa77787ff9ec015877dbd1f-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>9</td>
<td><span style="color:blue;">  EEG2Video </span></td>
<td>NeurIPS 2024</td>
<td>EEG2Video: Towards Decoding Dynamic Visual Perception from EEG</td>
<td>构建EEG视频对数据集，序列对齐EEG和视频</td>
<td>首次实现动态视频类别从EEG中解码</td>
<td>EEG→视频解码</td>
<td>未知</td>
<td>✅<a href="https://github.com/XuanhaoLiu/EEG2Video">GitHub</a>(80)</td>
<td><a href="https://openreview.net/forum?id=RfsfRn9OFd">链接</a></td>
</tr>
<tr>
<td>10</td>
<td>seegnificant</td>
<td>NeurIPS 2024</td>
<td>Neural Decoding from Stereotactic EEG: Accounting for Electrode Variability Across Subjects</td>
<td>编码每个电极为token，并做空间注意力融合</td>
<td>支持多受试者sEEG脑解码，良好迁移能力</td>
<td>sEEG行为预测</td>
<td>3</td>
<td>✅<a href="https://github.com/gmentz/seegnificant">GitHub</a>(15)</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/c473b9c8897f50203fa23570687c6b30-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>11</td>
<td>CADDA</td>
<td>ICLR 2022</td>
<td>CADDA: Class‑wise Automatic Differentiable Data Augmentation for EEG Signals</td>
<td>可微分的类条件数据增强策略</td>
<td>快速收敛，提高睡眠分期模型准确率</td>
<td>EEG分类</td>
<td>12</td>
<td>✅<a href="https://github.com/cliffordlab/sleep-convolutions-tf?utm_source=catalyzex.com">GitHub</a>(8)</td>
<td><a href="https://openreview.net/forum?id=6IYp-35L-xJ">链接</a></td>
</tr>
<tr>
<td>12</td>
<td>GNN-Seizure</td>
<td>ICLR 2022</td>
<td>Self-Supervised GNN for EEG Seizure Detection</td>
<td>图神经网络 + 自监督预测下一区段</td>
<td>癫痫发作识别效果大幅超越传统方法</td>
<td>EEG癫痫检测</td>
<td>10</td>
<td>✅<a href="https://github.com/tsy935/eeg-gnn-ssl">GitHub</a>(169)</td>
<td><a href="https://openreview.net/forum?id=k9bx1EfHI_-">链接</a></td>
</tr>
<tr>
<td>13</td>
<td>BrainBERT</td>
<td>ICLR 2023</td>
<td>BrainBERT: Self-supervised Learning for iEEG</td>
<td>iEEG语料无监督预训练Transformer</td>
<td>支持跨受试者的迁移学习，表现优异</td>
<td>iEEG多任务</td>
<td>8</td>
<td>✅<a href="https://github.com/czlwang/BrainBERT?utm_source=catalyzex.com">GitHub</a>(59)</td>
<td><a href="https://openreview.net/forum?id=xmcYx_reUn6">链接</a></td>
</tr>
<tr>
<td>14</td>
<td><span style="color:blue;"> Perceptogram</span></td>
<td>ICLR 2025</td>
<td>Perceptogram: Reconstructing Visual Percepts from EEG</td>
<td>使用CLIP嵌入和扩散模型重建图像</td>
<td>强可解释性语义图像生成，重建效果优异</td>
<td>EEG→图像生成</td>
<td>2</td>
<td>✅<a href="https://github.com/desa-lab/Perceptogram">GitHub</a>(32)</td>
<td><a href="https://openreview.net/forum?id=IZOeRDS6zU">链接</a></td>
</tr>
<tr>
<td>15</td>
<td><span style="color:blue;">LaBraM</span></td>
<td>ICLR 2024</td>
<td>Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI</td>
<td>LaBraM是一种统一的EEG基础模型，通过将EEG信号分割为通道片段，并使用向量量化神经谱预测技术训练神经分词器，将连续的原始EEG通道片段编码为紧凑的神经代码。随后，使用神经Transformer对这些神经代码进行预训练，以实现跨数据集的学习和通用的感知能力。</td>
<td>1.提出一种统一的EEG基础模型LaBraM，实现跨数据集的学习。 2.使用向量量化神经谱预测技术训练神经分词器，提升EEG信号的表示能力。   3.多个下游任务中表现优异，展示了模型的泛化能力。</td>
<td>异常检测、事件类型分类、情绪识别、步态预测等</td>
<td>92</td>
<td>✅<a href="https://github.com/935963004/LaBraM">GitHub</a>(387)</td>
<td><a href="https://openreview.net/forum?id=QzTpTRVtrP">OpenReview</a></td>
</tr>
<tr>
<td>16</td>
<td>EEG-ImageNet</td>
<td>ICLR 2025</td>
<td>EEG‑ImageNet: Dataset and Benchmarks with ImageNet Stimuli</td>
<td>大规模图像刺激EEG数据库，含多种标签</td>
<td>建立图像识别/重建基准，公开数据集</td>
<td>EEG→图像分类/重建</td>
<td>未知</td>
<td>✅<a href="https://github.com/Promise-Z5Q2SQ/EEG-ImageNet-Dataset">GitHub</a>(49)</td>
<td><a href="https://openreview.net/forum?id=ejVuTFFkl6">链接</a></td>
</tr>
<tr>
<td>17</td>
<td>Meta-MI</td>
<td>ICLR 2023</td>
<td>Meta‑Learning for Subject Adaptation in Low‑Data Environments for EEG‑Based MI BCI</td>
<td>元学习+少样本迁移方案</td>
<td>快速适应新受试者，提高泛化能力</td>
<td>EEG运动想象分类</td>
<td>15</td>
<td>✅<a href="https://github.com/arnav-pati/iclr2023-meta-learning-eeg-mi-classification">GitHub</a>(15)</td>
<td><a href="https://openreview.net/forum?id=7QqlQW9hJ8J">链接</a></td>
</tr>
<tr>
<td>18</td>
<td><span style="color:blue;">NeuroLM</span></td>
<td>ICLR 2025</td>
<td>NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals</td>
<td>将EEG信号视为一种外语，利用大型语言模型（LLMs）进行多任务学习，通过文本对齐的神经tokenizer和多通道自回归预训练，实现EEG与语言的统一建模。</td>
<td>1）首次将EEG信号整合到LLMs框架中，实现多任务学习；2）引入文本对齐的神经tokenizer，弥合EEG与文本之间的差距；3）在六个不同的EEG任务中展示出强大的泛化能力。</td>
<td>异常检测、事件类型分类、情绪识别、睡眠阶段分类、认知负荷预测等</td>
<td>15</td>
<td>✅<a href="https://github.com/935963004/neurolm">GitHub</a>(61)</td>
<td><a href="https://arxiv.org/abs/2409.00101">arXiv</a></td>
</tr>
<tr>
<td>19</td>
<td><span style="color:blue;">SBLEST</span></td>
<td>TPAMI 2023</td>
<td>Sparse Bayesian Learning for End-to-End EEG Decoding</td>
<td>多通道稀疏贝叶斯分类器，自动选择特征</td>
<td>跨任务解码能力强，具有神经可解释性</td>
<td>EEG分类（MI/ERP）</td>
<td>59</td>
<td>✅<a href="https://github.com/EEGdecoding/Code-SBLEST">GitHub</a>(43)</td>
<td><a href="https://doi.org/10.1109/TPAMI.2023.3299568">链接</a></td>
</tr>
<tr>
<td>20</td>
<td>CSLP-AE</td>
<td>NeurIPS 2023</td>
<td>CSLP‑AE: A Contrastive Split‑Latent Permutation Autoencoder Framework for Zero‑Shot EEG Signal Conversion</td>
<td>自监督学习EEG内容-风格分离表示，用于信号重建与转换</td>
<td>实现EEG任务/被试之间的零样本迁移学习</td>
<td>EEG信号转换</td>
<td>2</td>
<td>✅ <a href="https://github.com/andersxa/CSLP-AE">GitHub</a>(13)</td>
<td><a href="https://neurips.cc/virtual/2023/poster/72228">链接</a></td>
</tr>
<tr>
<td>21</td>
<td>Brant</td>
<td>NeurIPS 2023</td>
<td>Brant: Foundation Model for Intracranial Neural Signal</td>
<td>自监督训练的神经基础模型，基于Transformer处理iEEG</td>
<td>支持多个下游任务（预测、插补、癫痫检测等），具良好泛化性</td>
<td>颅内EEG（多任务）</td>
<td>4</td>
<td>✅ <a href="https://github.com/yzz673/Brant">GitHub</a>(29)</td>
<td><a href="https://proceedings.neurips.cc/paper/2023/hash/535915d26859036410b0533804cee788-Abstract-Conference.html">链接</a></td>
</tr>
<tr>
<td>22</td>
<td>DDNN（Differential Deep Neural Network）</td>
<td>TPAMI, 2022</td>
<td>Deep Learning Adapted to Differential Neural Networks Used as Pattern Classification of Electrophysiological Signals</td>
<td>提出一种基于微分神经网络（DDNN）的深度学习架构，用于分类执行图形识别测试的志愿者的脑电信号。该方法通过Lyapunov分析推导学习法则，确保分类误差和权重的局部渐近收敛。</td>
<td>构建了一个三层DDNN拓扑结构，展示了从单层82%到三层100%的分类准确率增长，验证了该架构在处理生物电信号分类任务中的有效性。</td>
<td>脑电信号分类、图形识别任务</td>
<td>10</td>
<td>暂无</td>
<td><a href="https://ieeexplore.ieee.org/document/9381613">链接</a></td>
</tr>
<tr>
<td>23</td>
<td>CLIP-KD EEG Decoder</td>
<td>ICLR 2024</td>
<td>Decoding EEG Signals of Visual Brain Representations with a CLIP-Based Knowledge Distillation</td>
<td>利用CLIP教师模型蒸馏训练EEG图像分类与生成网络</td>
<td>在EEG→图像分类与生成任务中显著优于传统方法</td>
<td>EEG图像分类/生成</td>
<td>2</td>
<td>❌</td>
<td><a href="https://iclr.cc/virtual/2024/23553">链接</a></td>
</tr>
<tr>
<td>24</td>
<td>Vision-Brain Align</td>
<td>ICLR 2024</td>
<td>Towards Neural Foundation Models for Vision: Aligning EEG, MEG and fMRI Representations</td>
<td>脑信号与图像表征的跨模态对齐框架</td>
<td>跨EEG/MEG/fMRI对图像进行语义编码/解码</td>
<td>脑信号↔图像表征映射</td>
<td>2</td>
<td>❌</td>
<td><a href="https://iclr.cc/virtual/2024/22525">链接</a></td>
</tr>
<tr>
<td>25</td>
<td>YOAS</td>
<td>TPAMI 2024</td>
<td>Brain-Inspired Image Perceptual Quality Assessment based on EEG: A QoE Perspective</td>
<td>从稀疏导联EEG生成稠密EEG并用于图像感知质量评估，引入Biased-EEGGanFormer与BiasEEGDiffFormer模块</td>
<td>基于EEG的图像质量评估新范式，结合GAN和扩散模型</td>
<td>图像感知质量评估</td>
<td>4</td>
<td>❌</td>
<td><a href="https://doi.org/10.48550/arXiv.2406.15269">链接</a></td>
</tr>
<tr>
<td>26</td>
<td>可解释睡眠分期网络</td>
<td>TPAMI 2024</td>
<td>A Multi-Level Interpretable Sleep Stage Scoring System by Infusing Experts' Knowledge Into a Deep Network Architecture</td>
<td>引入专家可解释卷积核的深度网络，支持多层次可视化解释</td>
<td>可解释性强，结合专家先验规则实现性能与可解释性统一</td>
<td>EEG睡眠分期</td>
<td>5</td>
<td>❌</td>
<td><a href="https://doi.org/10.48550/arXiv.2207.04585">链接</a></td>
</tr>
<tr>
<td>27</td>
<td><span style="color:orange;">EEG Foundation Challenge</span></td>
<td>NeurIPS 2025</td>
<td>EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding</td>
<td>提供多任务EEG数据与跨任务/受试挑战，促进基础模型研究</td>
<td>多任务/跨受试EEG建模挑战任务，支持精神病理预测</td>
<td>EEG泛化建模、精神病理预测</td>
<td>未知</td>
<td>✅<a href="https://eeg2025.github.io">GitHub</a></td>
<td><a href="https://doi.org/10.48550/arXiv.2506.19141">链接</a></td>
</tr>
<tr>
<td>28</td>
<td>SS-GNN</td>
<td>ICLR 2022</td>
<td>Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis</td>
<td>自监督图神经网络结合结构建图和发作预测</td>
<td>明显提升癫痫发作检测与解释性定位</td>
<td>EEG癫痫检测与解释</td>
<td>141</td>
<td>✅<a href="https://github.com/tsy935/eeg-gnn-ssl">GitHub</a>(169)</td>
<td><a href="https://doi.org/10.48550/arXiv.2104.08336">链接</a></td>
</tr>
<tr>
<td>29</td>
<td>EEG初始片段分类器</td>
<td>ICASSP 2022</td>
<td>Music identification using brain responses to initial snippets</td>
<td>利用 EEG 对应音乐初始片段（20s）的多通道功率谱特征训练分类器，实现后续歌曲片段的识别</td>
<td>验证了仅用初始 EEG 片段即可准确识别歌曲；发现β/γ频段特征效果最佳，并揭示了不同受试者对相同刺激的 EEG 响应差异</td>
<td>EEG→音乐ID分类</td>
<td>17</td>
<td>❌</td>
<td><a href="https://doi.org/10.1109/ICASSP43922.2022.9747332">链接</a></td>
</tr>
<tr>
<td>30</td>
<td>DAN 网络 (Dense Attention)</td>
<td>ICASSP 2022</td>
<td>Recognition of Silently Spoken Word from EEG Signals using Dense Attention Network (DAN)</td>
<td>提出密集注意力网络 (DAN)，在时域、频域和空间（电极）维度对 EEG 信号应用自注意力机制来提取特征。</td>
<td>DAN 在静默语音单词识别任务上达成 LTO 80.7%、LSO 75.1% 的识别率，优于其他方法。</td>
<td>EEG→静默语音单词分类</td>
<td>未知</td>
<td>❌</td>
<td><a href="https://doi.org/10.1109/ICASSP43922.2022.9746241">链接</a></td>
</tr>
<tr>
<td>31</td>
<td>浅-深层注意力网络 (SDANet)</td>
<td>ICASSP 2023</td>
<td>Relate auditory speech to EEG by shallow-deep attention-based network</td>
<td>提出 SDANet，使用注意力相关模块（ACM）从全局捕捉语音与 EEG 的相关性，通过浅层-深层相似性分类模块（SDSCM）结合浅层和深层特征进行分类。</td>
<td>在 Auditory EEG 挑战的 match-mismatch 任务中，该模型较基线实现了显著性能提升。</td>
<td>EEG→语音匹配分类</td>
<td>3</td>
<td>❌</td>
<td><a href="https://doi.org/10.48550/arXiv.2303.10897">链接</a></td>
</tr>
<tr>
<td>32</td>
<td><span style="color:blue;">HappyQuokka 系统 (FFT+全局调节) </span></td>
<td>ICASSP 2023 (SPGC)</td>
<td>HappyQuokka System for ICASSP 2023 Auditory EEG Challenge</td>
<td>提出基于 Transformer 的前馈网络（FFT）架构，并引入全局调节器提供个体信息辅助建模。</td>
<td>在 Auditory EEG 挑战的语音包络回归任务中获第一名，在预测单个受试者任务上显著优于基线。</td>
<td>EEG→语音包络回归</td>
<td>11</td>
<td>✅ <a href="https://github.com/jkyunnng/HappyQuokka_system_for_EEG_Challenge">GitHub</a>(5)</td>
<td><a href="https://doi.org/10.48550/arXiv.2305.06806">链接</a></td>
</tr>
<tr>
<td>33</td>
<td>复合神经解码器 (低频+γ带)</td>
<td>ICASSP 2024 (SPGC)</td>
<td>Detecting gamma‑band responses to the speech envelope for the ICASSP 2024 Auditory EEG Decoding SPGC</td>
<td>构建低频语音包络追踪解码器与 γ 波段响应解码器，并通过线性判别结合两者输出。</td>
<td>在 match-mismatch 任务中取得第一名，验证了高频 γ 带 EEG 对语音包络的响应具有高信噪比。</td>
<td>EEG→语音匹配分类</td>
<td>1</td>
<td>❌</td>
<td><a href="https://doi.org/10.48550/arXiv.2401.17380">链接</a></td>
</tr>
<tr>
<td>34</td>
<td>Naturalistic Music Decoding via Latent Diffusion</td>
<td>ICASSP 2025</td>
<td>Naturalistic Music Decoding from EEG Data via Latent Diffusion Models</td>
<td>利用 EEG‑conditioned ControlNet + AudioLDM2 latent diffusion，从原始 EEG 重构复杂自然音乐，无需手动预处理</td>
<td>首次实现端到端自然istic 音乐重建；提出基于 CLAP 和 EnCodec 的神经嵌入评价指标；显著优于传统 conv-baseline</td>
<td>EEG→音乐重建 / BCI 音频重构</td>
<td>4</td>
<td>❌</td>
<td><a href="https://arxiv.org/abs/2405.09062">链接</a></td>
</tr>
<tr>
<td>35</td>
<td>R&amp;B – Rhythm and Brain</td>
<td>KDD-AIDSH 2024 (Oral)</td>
<td>R&amp;B – Rhythm and Brain: Cross‑subject Decoding of Music from Human Brain Activity</td>
<td>利用 GTZan fMRI 数据，提取 10 种音乐流派对应的 CLAP 嵌入，采用功能 + 解剖对齐方式构建跨被试线性检索模型</td>
<td>实现跨被试音乐检索识别；凸显 CLAP 潜空间与脑表征对齐能力；识别关键脑区（STG、planum temporale、IP）</td>
<td>fMRI→音乐检索 / Cross‑subject 解码</td>
<td>1</td>
<td>✅ <a href="https://github.com/neoayanami/fmri-music-retrieve">GitHub</a>(2)</td>
<td><a href="https://arxiv.org/abs/2406.15537">链接</a></td>
</tr>
<tr>
<td>36</td>
<td>SSM2Mel</td>
<td>ICASSP 2025</td>
<td>SSM2Mel: State Space Model to Reconstruct Mel Spectrogram from the EEG</td>
<td>提出基于状态空间模型（SSM）的框架，结合S4-UNet结构和Mamba模块，有效重建EEG对应的梅尔频谱图</td>
<td>使用Embedding Strength Modulator（ESM）引入个体特征，在SparrKULee数据集上表现优异（ρ=0.069）</td>
<td>EEG→语音重建</td>
<td>未知</td>
<td>✅ <a href="https://github.com/fchest/SSM2Mel">GitHub</a>(0)</td>
<td><a href="https://arxiv.org/abs/2501.10402">链接</a></td>
</tr>
<tr>
<td>37</td>
<td><span style="color:blue;">NICE‑EEG </span></td>
<td>ICLR 2024</td>
<td>Decoding Natural Images from EEG for Object Recognition</td>
<td>利用对比学习对齐图像与EEG编码器，学习自然图像和EEG响应的共同表示</td>
<td>首次在自然图像级别实现 EEG→图像类别的零样本识别；验证脑电信号的时空谱语义可解释性</td>
<td>EEG→图像识别</td>
<td>34</td>
<td>✅ <a href="https://github.com/eeyhsong/NICE-EEG">GitHub</a>（157）</td>
<td>✅ <a href="https://openreview.net/forum?id=dhLIno8FmH">OpenReview</a>, <a href="https://arxiv.org/abs/2308.13234">arXiv</a></td>
</tr>
<tr>
<td>38</td>
<td><span style="color:blue;">Dual‑DualGAN‑Speech</span></td>
<td>Knowledge‑Based Systems 2023</td>
<td>End‑to‑end translation of human neural activity to speech with a dual–dual generative adversarial network</td>
<td>利用 Dual‑DualGAN 架构，端到端将 EEG 神经活动映射为语音波形；引入 transition 域结合 EEG 与语音信号进行训练</td>
<td>创新性地实现 EEG→语音的端到端翻译，并构建新 EEG‑speech 数据集；相较于之前方法显著提高句子层面合成质量</td>
<td>EEG→语音合成</td>
<td>12</td>
<td>✅ <a href="https://github.com/qwe1218088/dual-dualgan">GitHub</a>(4)</td>
<td>✅ <a href="https://personalpages.surrey.ac.uk/w.wang/papers/Guo%20et%20al_KBS_2023.pdf">PDF</a></td>
</tr>
<tr>
<td>39</td>
<td>EEG‑MAE</td>
<td>Augmented Cognition (HCII 2024)</td>
<td>Enhancing Representation Learning of EEG Data with Masked Autoencoders</td>
<td>使用 MAE 架构对 EEG 信号随机掩码并重构，预训练编码器后微调下游任务（如 EEGEyeNet 注视估计）</td>
<td>在注视估计任务中，预训练模型在训练时间减少至三分之一时仍保持性能，并整体提升学习效率 ()</td>
<td>EEG→注视估计（gaze estimation）</td>
<td>未知</td>
<td>✅<a href="https://github.com/YifeiZhou675/EEGViT-MAE">GitHub</a>(2)</td>
<td>✅ <a href="https://arxiv.org/abs/2408.05375">arXiv</a></td>
</tr>
<tr>
<td>40</td>
<td>Nonlinear 音乐重建</td>
<td>PLoS Biology 2023</td>
<td>Music can be reconstructed from human auditory cortex activity using nonlinear decoding models</td>
<td>使用 iEEG（ECoG）采集音频刺激，应用非线性回归模型重建音频频谱</td>
<td>首次从皮层 iEEG 重建整首音乐波形，揭示 STG 中节奏子区作用</td>
<td>ECoG → 音乐重建</td>
<td>39</td>
<td>❌</td>
<td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10427021/">PMC</a></td>
</tr>
<tr>
<td>41</td>
<td>hvEEGNet</td>
<td>Journal of Neural Engineering 2023</td>
<td>hvEEGNet: a novel deep learning model for high-fidelity EEG reconstruction</td>
<td>层级变分自编码器结合 DTW loss 实现高保真 EEG 重建</td>
<td>通过 DTW 约束和 VAE 架构，实现 EEG 多通道快速高质量重建</td>
<td>EEG → EEG（信号重构）</td>
<td>未知</td>
<td>❌</td>
<td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11695360/">PMC</a></td>
</tr>
<tr>
<td>42</td>
<td>Melodic expectations TRF</td>
<td>Eur J Neurosci 2024</td>
<td>Neural encoding of melodic expectations in music across EEG frequency bands</td>
<td>使用 TRF 模型，将 EEG 信号回归为音乐旋律预测信息（熵与意外性）</td>
<td>表明低频EEG编码节奏/旋律信息，音乐专业者与非专业者在频段上差异显著</td>
<td>EEG → 音乐预测/回归</td>
<td>1</td>
<td>❌</td>
<td><a href="https://pubmed.ncbi.nlm.nih.gov/39469882/">PubMed</a></td>
</tr>
<tr>
<td>43</td>
<td>Imagined Speech 解码</td>
<td>eNeuro 2024</td>
<td>Imagined speech can be decoded from low- and cross-frequency intracranial EEG features</td>
<td>分析低频与高频 EEG 脑信号跨频特性，进行想象语音解码</td>
<td>在sEEG/MEG上实现非侵入式脑机接口预测隐藏语音</td>
<td>iEEG → 想象语音回归</td>
<td>40</td>
<td>❌</td>
<td><a href="https://pubmed.ncbi.nlm.nih.gov/35013268/">PubMed</a></td>
</tr>
<tr>
<td>44</td>
<td><span style="color:blue;">ConvConcatNet</span></td>
<td>ICASSP 2024 Auditory EEG Challenge</td>
<td>ConvConcatNet: A Deep Convolutional Neural Network to Reconstruct Mel Spectrogram from the EEG</td>
<td>￼	深度卷积神经网络，在每个块中迭代地将中间特征与原始EEG输入进行拼接，并应用空间注意力到通道￼。学习从EEG到语音梅尔谱的非线性映射。</td>
<td>提出ConvConcatNet架构：结合深度卷积和多次拼接操作，将各层特征与原始EEG拼接，并加入空间注意力，显著提升谱重建相关性 ￼。在Task 2中达到了竞赛最高相关度（ρ=0.0420） ￼。</td>
<td>EEG→语音重构（语音解码）</td>
<td>7</td>
<td>✅<a href="https://github.com/xuxiran/ConvConcatNet">GitHub</a>(4) ￼</td>
<td><a href="https://arxiv.org/abs/2401.04965">arXiv</a></td>
</tr>
<tr>
<td>45</td>
<td><span style="color:blue;"> Self-Supervised Multi-Feature Contrastive Model	</span></td>
<td>ICASSP 2024 Auditory EEG Challenge</td>
<td>Self-Supervised Speech Representation and Contextual Text Embedding for Match-Mismatch Classification with EEG Recording</td>
<td>￼深度卷积神经网络（Deep CNN）提取脑电图（EEG）特征；语音特征包括wav2vec2.0嵌入和基于GPT的语音文本上下文。对比学习框架对齐EEG和多层次语音特征。特征经过融合和集成处理。 ￼</td>
<td>创新地结合多级语音特征（低级的包络、Mel谱，高级的wav2vec2和GPT文本嵌入）与EEG进行对比学习 ￼。通过特征融合与模型集成达到60.29%分类准确率（Task 1第二名） ￼。</td>
<td>EEG→语音匹配分类</td>
<td>1</td>
<td>✅<a href="https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch">GitHub</a>(13)</td>
<td>￼	<a href="https://arxiv.org/abs/2401.04964">arXiv</a> ￼ ￼</td>
</tr>
<tr>
<td>46</td>
<td><span style="color:blue;"> 	Cross-Attention-Guided WaveNet	</span></td>
<td>ICASSP 2024 Auditory EEG Challenge</td>
<td>Cross-Attention-Guided WaveNet for EEG-to-MEL Spectrogram Reconstruction in the ICASSP 2024 Auditory EEG Challenge</td>
<td>￼基于WaveNet的新型模型，采用粗到细的重建策略。该模型依次从EEG信号中重建语音包络、10带梅尔频谱、80带梅尔频谱及幅度谱图。交叉注意力机制用于连接EEG与语音特征。采用Mixup数据增强技术及联合损失函数。</td>
<td>提出交叉注意力WaveNet框架 ￼：结合WaveNet时序建模与跨模态注意力，并使用粗细粒度策略（逐步预测Envelope→Mel10→Mel80→Mag） ￼ ￼。引入Mixup等技术提高泛化。实现验证集Pearson ρ≈0.0651，测试集0.0413（竞赛第二名） ￼ ￼。</td>
<td>EEG→语音重构、脑-机接口</td>
<td>1</td>
<td>✅<a href="https://github.com/IMU-FangYuan/Multi-Stage-Multi-Target-WaveNet-for-the-ICASSP-2024-Auditory-EEG-Challenge-2024">GitHub</a>(4) ￼</td>
<td><a href="https://www.isca-archive.org/interspeech_2024/li24ga_interspeech.pdf">Interspeech 2024 (Li et al.)</a> ￼ ￼</td>
</tr>
<tr>
<td>47</td>
<td><span style="color:blue;"> 	FESDE	</span></td>
<td>Interspeech 2024</td>
<td>Toward Fully-End-to-End Listened Speech Decoding from EEG Signals</td>
<td>提出FESDE框架，直接从EEG信号生成语音波形，省略中间声学特征处理步骤。</td>
<td>首次实现从EEG信号直接重建听觉语音波形，简化解码流程，提高效率。</td>
<td>听觉语音解码、脑-机接口语音合成</td>
<td>2</td>
<td>✅<a href="https://github.com/lee-jhwn/fesde">GitHub1</a>(22) ✅<a href="https://github.com/lee-jhwn/icassp25-fesde-phoneme">GitHub2</a>(3)</td>
<td><a href="https://arxiv.org/abs/2406.08644">arXiv</a></td>
</tr>
<tr>
<td>48</td>
<td>ATDA</td>
<td>NeurIPS 2024</td>
<td>Adversarial Training based Domain Adaptation for Cross-Subject Emotion Recognition</td>
<td>提出基于对抗训练的领域自适应方法，通过通道和特征注意力机制，提取情感特异性和领域不变特征。</td>
<td>在SEED数据集上显著提升跨被试情感识别性能，缓解领域偏移问题。</td>
<td>情感识别、跨被试脑机接口</td>
<td>未知</td>
<td>❌</td>
<td><a href="https://openreview.net/forum?id=TLjxEcp1PK">OpenReview</a></td>
</tr>
<tr>
<td>49</td>
<td>Hierarchical-ViT</td>
<td>NeurIPS 2024</td>
<td>Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with EEG-Vision Transformer</td>
<td>提出Hierarchical-ViT框架，结合层次化视觉特征提取、EEG-ViT处理和基于CLIP的联合学习，实现高质量的图像重建。</td>
<td>提升了EEG信号驱动的图像重建质量和精度，捕捉复杂视觉特征。</td>
<td>图像重建、脑视觉解码</td>
<td>未知</td>
<td>❌</td>
<td><a href="https://openreview.net/forum?id=SjXJOsLi1X#discussion">OpenReview</a></td>
</tr>
<tr>
<td>50</td>
<td><span style="color:blue;"> BrainVis</span></td>
<td>ICASSP 2025</td>
<td>BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction</td>
<td>提出BrainVis方法，结合自监督的EEG时域特征提取和频域特征增强，将EEG嵌入与CLIP语义空间对齐，并通过级联扩散模型重建图像。</td>
<td>在仅使用10%训练数据的情况下，超越了现有方法的语义保真度和生成质量。</td>
<td>脑-视觉解码、图像重建</td>
<td>7</td>
<td>✅<a href="https://github.com/RomGai/BrainVis">GitHub</a>(58)</td>
<td><a href="https://arxiv.org/abs/2312.14871">arXiv</a></td>
</tr>
<tr>
<td>51</td>
<td><span style="color:orange;">ICASSP 2023 Auditory EEG Decoding Challenge</span></td>
<td>ICASSP 2023</td>
<td>ICASSP 2023 AUDITORY EEG DECODING CHALLENGE</td>
<td>本文介绍了ICASSP 2023听觉EEG解码挑战赛，包括两个任务：1）匹配-不匹配任务，判断给定的EEG片段与两个语音片段中的哪一个匹配；2）回归任务，从EEG信号中重建语音包络。</td>
<td>提供了一个包含85名受试者、总计157小时数据的大型听觉EEG数据集，并定义了两个标准任务，促进了EEG与语音信号之间关系的研究。</td>
<td>语音匹配-不匹配分类、语音包络重建</td>
<td>N/A</td>
<td>✅<a href="https://github.com/exporl/auditory-eeg-challenge-2023-code">GitHub</a>(14)</td>
<td><a href="https://sigport.org/sites/default/files/docs/auditory_eeg_decoding_challenge.pdf">链接</a></td>
</tr>
<tr>
<td>52</td>
<td><span style="color:orange;">ICASSP 2024 Auditory EEG Decoding Challenge</span></td>
<td>ICASSP 2024</td>
<td>Auditory EEG decoding challenge for ICASSP 2024</td>
<td>本文介绍了ICASSP 2024听觉EEG解码挑战赛，任务包括：1）匹配-不匹配任务，给定一个EEG片段和五个语音片段，判断哪一个语音片段与EEG片段匹配；2）回归任务，从EEG信号中重建Mel频谱图。</td>
<td>提供了一个包含105名受试者、总计188小时数据的大型听觉EEG数据集，任务难度相较前一届有所增加，推动了EEG与语音信号之间更复杂关系的研究。</td>
<td>语音匹配-不匹配分类、Mel频谱图重建</td>
<td>N/A</td>
<td>✅<a href="https://github.com/exporl/auditory-eeg-challenge-2024-code">GitHub</a>(31)</td>
<td><a href="https://d197for5662m48.cloudfront.net/documents/publicationstatus/221018/preprint_pdf/18a9744752f4cb39ac5597c2c3defaf1.pdf">链接</a></td>
</tr>
<tr>
<td>53</td>
<td>Music-Affect-MDA</td>
<td>ICASSP 2022</td>
<td>Enhancing Affective Representations of Music-Induced EEG through Multimodal Supervision and Latent Domain Adaptation</td>
<td>构建双流网络，一路通过 LSTM-attention 处理 EEG，另一路利用预训练音乐标签模型提取音乐特征，并通过反向域判别器对齐两种模态，同时融合情绪标签进行监督。</td>
<td>提出跨模态对齐策略，将音乐语义信息注入 EEG 表征；在 DEAP 数据集上提升情绪分类性能，同时支持 EEG → 音乐片段检索。</td>
<td>音乐情绪识别、EEG → 音乐片段检索</td>
<td>6</td>
<td>✅<a href="https://github.com/klean2050/EEG_CrossModal">GitHub</a>(9)</td>
<td><a href="https://arxiv.org/abs/2202.09750">arXiv</a></td>
</tr>
<tr>
<td>54</td>
<td><span style="color:blue;">EEG2Mel</span></td>
<td>InterSpeech 2022</td>
<td>EEG2Mel: Reconstructing Sound from Brain Responses to Music</td>
<td>使用 CNN 对齐 EEG 时间窗口与音乐谱图，直接回归 mel 频谱图，一步生成可辨识音乐</td>
<td>在 NMED-Tempo/Hindi 数据集上对使用电压与频谱输入进行实验，mel 频谱重建分类准确率 81%，重建音乐可识别度达 85%</td>
<td>EEG→音乐重建（mel 频谱）</td>
<td>16</td>
<td>✅ <a href="https://github.com/AGRamirezz/Decoding-Neural-Activity-to-Reconstruct-and-Generate-Music-Stimuli">GitHub</a>(17)</td>
<td><a href="https://arxiv.org/abs/2207.13845">arXiv:2207.13845</a></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

</body>
</html>
